{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Analytics - Assignment 1 {-}\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment Points**: 100  \n",
    "**Due Date**: Friday Week 4 (17 March) @ 11.59pm  \n",
    "**Submission**: Submit your file using the submission link on iLearn\n",
    "\n",
    "\n",
    "- Put **all your work** into this file;\n",
    "- Failure to supply solutions in the cells provided below each question will result in a loss of 30 points;\n",
    "- Follow all instructions closely and **do not** print your variables to the screen unless explicitly asked to do so;\n",
    "    - Comment out print statements where necessary and make sure that your submitted notebook does not have the output of previously executed print statements;\n",
    "    - 10 marks will be deducted for every redundant print statement not explicitly asked for.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Assignment\n",
    "\n",
    "Credit score cards are used as a risk control method in the financial industry. Personal information submitted by credit card applicants are used to predict the probability of future defaults. The bank employs such data to decide whether to issue a credit card to the applicant or not.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "| Feature Name         | Explanation     | Additional Remarks |\n",
    "|--------------|-----------|-----------|\n",
    "| ID | Randomly allocated client number      |         |\n",
    "| Income   | Annual income  |  |\n",
    "| Gender   | Applicant's Gender   | Male = 0, Female = 1  |\n",
    "| Car | Car Ownership | Yes = 1, No = 0 | \n",
    "| Children | Number of Children | |\n",
    "| Real Estate | Real Estate Ownership | Yes = 1, No = 0 \n",
    "| Days Since Birth | No. of Days | Count backwards from current day (0), -1 means yesterday\n",
    "| Days Employed | No. of Days | Count backwards from current day(0). If positive, it means the person is currently unemployed.\n",
    "| Payment Default | Whether a client has overdue credit card payments | Yes = 1, No = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Problem 1 - (50 points) {-}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1** \n",
    "\n",
    "- Import the `assignment_data.xlsx` file from `data` folder into a pandas DataFrame named `df`; \n",
    "- Delete duplicate rows from `df` according to `ID`;\n",
    "- Delete the `ID` column.\n",
    "- How many rows are left in `df`?\n",
    "\n",
    "(10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5976"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('assignment_data.xlsx')\n",
    "df = df.drop_duplicates(subset = 'ID', keep=\"first\") \n",
    "df = df.drop(['ID'], axis = 1)\n",
    "#len(df) ---> number of remaining rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5976 rows left in the Dataframe after dropping duplicate ID's.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "- Reset the index in `df` using an appropriate function from `pandas` so that the new index corresponds to the number of rows (make sure to delete the old index). \n",
    "- How many positive values of `Days Employed` are there?\n",
    "- Replace the positive values of `Days Employed` with 0 (zero) in `df`\n",
    "\n",
    "(10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResetDF = df.reset_index()\n",
    "ResetDF = ResetDF.drop(['index'], axis = 1)\n",
    "# print(sum(ResetDF['Days Employed']>0)) No. of instances of unemployment\n",
    "ResetDF['Days Employed'] = ResetDF['Days Employed'].apply(lambda x: 0 if x>0 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were 967 positive values for Days Employed, meaning there were 967 instances of unemployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Question 3**\n",
    "\n",
    "Create two new variables in `df` named \n",
    "\n",
    "1. `Age`;\n",
    "2. `Years in Employment`,\n",
    "\n",
    "which measure age and employment length in **years** (decimal numbers) from `Days Since Birth` and `Days Employed` by applying approapriate transformations on these variables. \n",
    "\n",
    "Delete the original variables `Days Since Birth` and `Days Employed`.\n",
    "\n",
    "(5 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResetDF['Age'] = ResetDF.apply(lambda x: abs(x['Days Since Birth'])/365, axis = 1)\n",
    "ResetDF['Years in Employment'] = ResetDF.apply(lambda x: abs(x['Days Employed'])/365, axis = 1)\n",
    "ResetDF=ResetDF.drop(['Days Since Birth'], axis = 1)\n",
    "ResetDF=ResetDF.drop(['Days Employed'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Question 4**\n",
    "\n",
    "- Create a **one**-dimensional NumPy array named `y` by exporting the first 5,000 observations of `Payment_Default`. (Hint: see `ravel()` function)\n",
    "- Create a NumPy array named `X` by exporting the first 5,000 observations of the following columns `Gender`, `Car`, `Real Estate`, `Children`, `Income`, `Age`, `Years in Employment`.\n",
    " \n",
    "(10 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y = np.ravel(ResetDF.loc[:4999]['Payment Default'], order = 'C')\n",
    "X = ResetDF.loc[:4999][['Gender','Car','Real Estate','Children','Income','Age','Years in Employment']].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Question 5** \n",
    "\n",
    "- Use an appropriate `scikit-learn` library we learned in class to create the following NumPy arrays: `y_train`, `y_test`, `X_train` and `X_test` by splitting the data into 70% training and 30% test datasets. \n",
    "- Set `random_state` to 0 and stratify subsamples so that train and test datasets have roughly equal proportions of the target's class labels. \n",
    "\n",
    "(5 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Question 6**\n",
    "\n",
    "- Create new variables by using an appropriate `scikit-learn` library we learned in class to standardize the features from the training and test datasets to mean zero and variance one. Name the new variables by appending '_scaled' to the original variable names.\n",
    "\n",
    "\n",
    "(10 points)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "np.set_printoptions(precision=3, suppress = True) # pretty printing\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "\n",
    "X_train_scaled = sc.transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 2 - (20 Points) {-}\n",
    "\n",
    "**Question 7**\n",
    "\n",
    "Fit the following two classifiers to the transformed training dataset using `scikit-learn` libraries.\n",
    "\n",
    "- Perceptron - name your instance `pc` set `random_state=1`\n",
    "- Logistic Regression - name your instance `lr` set `random_state=1`\n",
    "\n",
    "When initializing instances of the above classifiers only set the parameters referenced above and nothing else.\n",
    "\n",
    "(20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pc = Perceptron(random_state=1)\n",
    "pc.fit(X_train_scaled,y_train)\n",
    "\n",
    "lr = LogisticRegression(random_state=1)\n",
    "lr.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3 - (30 points) {-}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8**\n",
    "\n",
    "- Using a method built into each of the two classifiers compute their prediction accuracies on the training data;\n",
    "- Store the accuracy values into variables named according to the following pattern: `classifier_name_accuracy_train`, e.g. you should have `lr_accuracy_train`; \n",
    "- Print the two accuracy **variables** along with their brief descriptions.\n",
    "\n",
    "(10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC Train Accuracy = 0.496 \n",
      " The Perceptron classifer was able to predict Payment Default with respect to the independent variable in the training set with an accuracy of 49.6% \n",
      "\n",
      "LR Train Accuracy = 0.558 \n",
      " The Logistic Regression classifer was able to predict Payment Default with respect to the independent variable in the training set with an accuracy of 55.8%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pc_y_train_pred = pc.predict(X_train_scaled)\n",
    "pc_accuracy_train = (f'PC Train Accuracy = {accuracy_score(y_train, pc_y_train_pred):.3f}')\n",
    "\n",
    "lr_y_train_pred = lr.predict(X_train_scaled)\n",
    "lr_accuracy_train = (f'LR Train Accuracy = {accuracy_score(y_train, lr_y_train_pred):.3f}')\n",
    "\n",
    "print(pc_accuracy_train,\"\\n The Perceptron classifer was able to predict Payment Default with respect to the independent variable in the training set with an accuracy of 49.6% \\n\")\n",
    "print(lr_accuracy_train,\"\\n The Logistic Regression classifer was able to predict Payment Default with respect to the independent variable in the training set with an accuracy of 55.8%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Question 9** \n",
    "\n",
    "- Using a method built into each of the above classifiers compute their prediction accuracy for the test dataset\n",
    "- Store the accuracy values into variables named according to the following pattern: `classifier_name_accuracy_test`, e.g. you should have `lr_accuracy_test`. \n",
    "- Print the two accuracy **variables** along with brief descriptions.\n",
    "\n",
    "(10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC Test Accuracy = 0.495 \n",
      " The Perceptron classifer was able to predict Payment Default with respect to the independent variable in the test set with an accuracy of 49.6% \n",
      "\n",
      "LR Test Accuracy = 0.507 \n",
      " The Logistic Regression classifer was able to predict Payment Default with respect to the independent variable in the test set with an accuracy of 55.8%\n"
     ]
    }
   ],
   "source": [
    "pc_y_test_pred = pc.predict(X_test_scaled)\n",
    "pc_accuracy_test = (f'PC Test Accuracy = {accuracy_score(y_test, pc_y_test_pred):.3f}')\n",
    "\n",
    "\n",
    "lr_y_test_pred = lr.predict(X_test_scaled)\n",
    "lr_accuracy_test = (f'LR Test Accuracy = {accuracy_score(y_test, lr_y_test_pred):.3f}')\n",
    "\n",
    "print(pc_accuracy_test,\"\\n The Perceptron classifer was able to predict Payment Default with respect to the independent variable in the test set with an accuracy of 49.6% \\n\")\n",
    "print(lr_accuracy_test,\"\\n The Logistic Regression classifer was able to predict Payment Default with respect to the independent variable in the test set with an accuracy of 55.8%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Question 10** \n",
    "\n",
    "Using nicely formated text in Markdown comment on the accuracies computed in Questions 8 & 9 making sure you address:\n",
    "- training and test set datasets; \n",
    "- Perceptrion and Logistic Regression models. \n",
    "\n",
    "Are the results as expected, and why or why not? (Hint: You are not expected to comment on why a particular model is better.) \n",
    "\n",
    "(10 marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Payment Default Analysis\n",
    "\n",
    "To analyse the credit score for credit card applicants, two classifier methods were used to accurately predict Payment Default with respect to seven independent variables:\n",
    "  * Gender\n",
    "  * Car\n",
    "  * Real Estate\n",
    "  * Children\n",
    "  * Income\n",
    "  * Age\n",
    "  * Years in Employment\n",
    "  \n",
    "This was done throguh a split of 70/30 into a training and test data set that was fit for both a Perceptron and Logisitic Regression analysis. Logically it would make sense that the train set has a higher accuracy as there is more data for it use against payment default. This is done through assigning a value of 1 or 0 to a predicted y value and then matched with the actual value to see if it was correctly assigned a true (default) or false (no default).\n",
    "\n",
    "### Perceptron Analysis\n",
    "Whilst the Perceptron test had the lower accuracy for both the train and test data sets out of the two models, this model had the closest train and test accuracy with 49.6 and 49.5 percent respectively. This means that the Perceptron was able to correctly match predicted values with the actual values around 50% of the time. Having the training set have a higher accuracy to the test set was expected even though it was only 0.01 percent higher.\n",
    "\n",
    "### Logisitic Regression Analysis\n",
    "The Logisitc Regression model had a higher accuracy for both the train and test sets however had a larger disparity between the two, boasting 55.8 and 50.7 percent respectively, a differnce of 5.1%. This means that the Logisitic Regression was able to correctly match predicted values with the actual values around 50-55% of the time.This result was expected as for the aforementioned reasoning, and was separated more so than the Perceptron model.\n",
    "\n",
    "### Conlusion\n",
    "\n",
    "Overall, the training set for both models matched expectations obtaining a higher accuracy than their test counterparts. Depsite this, an accuracy of around 50-55 percent is relatively low and would not seem to \"accurately\" predict a payment default for credit card applicants. Perhaps more data is needed or a different set or predictors are required for a better fit for these models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
